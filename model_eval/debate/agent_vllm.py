from vllm import LLM, SamplingParams
from .agent_abc import AbstractLLMAgent
from .pareval_codeopt_prompts import *
from .debate_prompts import generate_code_opt_with_other_responses
from .php_prompts import generate_code_opt_with_one_hint
from .utils import *
import logging
import json
import jsonlines
from transformers import AutoTokenizer, AutoModel
from logging import Logger

class VLLMAgent(AbstractLLMAgent):
    def __init__(self, llm: LLM, model_name: str, logger: Logger, length_tokenizer: AutoTokenizer, additional_package: str, temperature: int =0.2, reason_temperature: int=0.7, top_p : int =0.95, frequency_penalty : int =0):
        """
        llm: VLLM server instance
        model_name: huggingface model repository name for inference
        """
        # Call the parent class's __init__ method
        super().__init__(llm)
        print(f"Initializing VLLM agent {model_name}")
        #self.additional_param = additional_param
        self.model_name = model_name
        self.context = {}
        self.memory = []
        self.single_round_memory = []
        self.logger = logger
        self.length_tokenizer = length_tokenizer
        self.additional_package = additional_package

        # NOTE: use config dictionary instead
        self.temperature=temperature
        self.top_p = top_p
        self.frequency_penalty = frequency_penalty

        self.reason_temperature = reason_temperature
    

    def generate_prompt(self, context: dict, task: str) -> str:
        """
        context: dictionary of context, the keys are:
            src_code: the source code yet to optimize
            tgt_code: the code optimized from source code by LLM
            feedback: execution feedback in the terminal
            lessons: the explanation generated by LLM
            issues: the issues of explanation identified by LLM

        task: str, has to be "CODE", "LESSON", "IDENTIFY" or "MODIFY, choose which prompt to generate

        return: generated prompt for different tasks
        """
        if task == "CODE":
            if context["lessons"] == "":
                prompt = generate_code_opt_prompt_code(src_code=context["src_code"], additional_package=self.additional_package)
            else:
                prompt = generate_code_opt_prompt_code_with_lessons(src_code=context["src_code"], lessons=context["lessons"], additional_package=self.additional_package)
        elif task == "LESSON":
            log, tag, speedup = pareval_process_execution_feedback(log=context["feedback"])
            if tag == "CORRECT":
                if speedup >= 1.0 and speedup < 1.1:
                    faster_or_slower = "slightly faster"
                elif speedup >= 1.1:
                    faster_or_slower = "significantly faster"
                else:
                    faster_or_slower = "slower"
                prompt = generate_lesson_correct_tgt_code_prompt(src_code=context["src_code"], tgt_code=context["tgt_code"], faster_or_slower=faster_or_slower, speedup=speedup)
            elif tag == "INCORRECT":
                prompt = generate_lesson_incorrect_tgt_code_prompt(src_code=context["src_code"], tgt_code=context["tgt_code"])
            elif tag == "NOT_COMPILABLE":
                log_lines = log.splitlines()  # Split log into lines
                log = "\n".join(log_lines[:7]) if len(log_lines) > 7 else "\n".join(log_lines)
                prompt = generate_lesson_not_compilable_tgt_code_prompt(src_code=context["src_code"], tgt_code=context["tgt_code"], feedback=log)
        elif task == "IDENTIFY":
            log, tag, speedup = pareval_process_execution_feedback(log=context["feedback"])
            
            if tag == "CORRECT":
                # Correct code
                if speedup > 1.0 and speedup < 1.1:
                    faster_or_slower = "slightly faster"
                elif speedup >= 1.1:
                    faster_or_slower = "significantly faster"
                else:
                    faster_or_slower = "slower"
                prompt = identify_lesson_correct_tgt_code(
                    src_code=context["src_code"],
                    tgt_code=context["tgt_code"],
                    faster_or_slower=faster_or_slower,
                    lesson=context["lesson"]
                )

            elif tag == "INCORRECT":
                # Incorrect code
                prompt = identify_lesson_incorrect_tgt_code(
                    src_code=context["src_code"],
                    tgt_code=context["tgt_code"],
                    lesson=context["lesson"]
                )

            elif tag == "NOT_COMPILABLE":
                # Not compilable code
                prompt = identify_lesson_non_compilable_tgt_code(
                    src_code=context["src_code"],
                    tgt_code=context["tgt_code"],
                    lesson=context["lesson"]
                )

        elif task == "MODIFY":
            log, tag, speedup = pareval_process_execution_feedback(log=context["feedback"])
            if tag == "CORRECT":
                # Correct code
                if speedup > 1.0 and speedup < 1.1:
                    faster_or_slower = "slightly faster"
                elif speedup >= 1.1:
                    faster_or_slower = "significantly faster"
                else:
                    faster_or_slower = "slower"
                prompt = modify_lesson_correct_tgt_code(
                src_code=context["src_code"],
                tgt_code=context["tgt_code"],
                faster_or_slower=faster_or_slower,
                lesson=context["lesson"],
                issues=context["issues"]
            )

            elif tag == "INCORRECT":
                prompt = modify_lesson_incorrect_tgt_code(
                src_code=context["src_code"],
                tgt_code=context["tgt_code"],
                lesson=context["lesson"],
                issues=context["issues"]
            )
            elif tag == "NOT_COMPILABLE":
                prompt = modify_lesson_non_compilable_tgt_code(
                src_code=context["src_code"],
                tgt_code=context["tgt_code"],
                lesson=context["lesson"],
                issues=context["issues"]
            )
        else:
            print(f"Not valid task {task}, such prompt is not valid.")
        
        
        return prompt


    def process_response(self, response) -> str:
        return clean_output(response)
    
    def infer(self, prompt: str) -> str:
        """
        Inference the LLM with vllm OpenAI API and vllm server 

        Args:
            prompt: the prompt that to use inference LLM.

        return: the response from LLM
        """

        in_tokens = len(self.length_tokenizer.tokenize(prompt))
        output = self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                top_p=self.top_p,
                frequency_penalty=self.frequency_penalty
            )

        output = output.choices[0].message.content
        out_tokens = len(self.length_tokenizer.tokenize(output))
        #optimized_code = clean_output(optimized_code)
        return output, in_tokens, out_tokens
    
    def reason(self, prompt: str, temperature: float, top_p: float, frequency_penalty: float, max_completion_tokens: int=250) -> str:
        """
        Inference the LLM with vllm OpenAI API and vllm server, and a different set of temperature for reasoning

        Args:
            prompt: the prompt that to use inference LLM.

        return: the response from LLM
        """
        in_tokens = len(self.length_tokenizer.tokenize(prompt))

        output = self.llm.chat.completions.create(
                model=self.model_name,
                messages=[
                {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                max_completion_tokens=max_completion_tokens
            )

        output = output.choices[0].message.content
        out_tokens = len(self.length_tokenizer.tokenize(output))

        #optimized_code = clean_output(optimized_code)
        return output, in_tokens, out_tokens
    
    def optimize_code(self, context: dict) -> str:
        """
        Perform Code Optimization by using LLM

        Args:
            context (Dict[any, any]): dictionary, containing necessary information to form a prompt

        Returns
            str: optimized code 
        """
        prompt = self.generate_prompt(context=context, task="CODE")
        #self.logger.info("Optimizer Prompt: \n%s", prompt)
        #self.logger.info("OPTIMIZER PROMPT END!!!")
        output, in_tokens, out_tokens = self.infer(prompt=prompt)
        optimized_code = clean_output(output)
        if optimized_code == "":
            print(f"No code block found for {self.model_name}")
        return optimized_code, in_tokens, out_tokens
    
    def optimize_code_with_other_codes(self, context: dict, other_codes: str) -> str:
        """
        Perform Code Optimization by using LLM

        Args:
            context (Dict[any, any]): dictionary, containing necessary information to form a prompt

        Returns
            str: optimized code 
        """
        
        prompt = generate_code_opt_with_other_responses(src_code=context["src_code"], other_codes=other_codes, additional_package=self.additional_package)
        # prompt = self.generate_prompt(context=context, task="CODE")
        #self.logger.info("Optimizer Prompt: \n%s", prompt)
        #self.logger.info("OPTIMIZER PROMPT END!!!")
        output, in_tokens, out_tokens = self.infer(prompt=prompt)
        optimized_code = clean_output(output)
        if optimized_code == "":
            print(f"No code block found for {self.model_name}")
        return optimized_code, in_tokens, out_tokens
    
    def optimize_code_with_hint(self, context: dict, other_codes: str) -> str:
        """
        Perform Code Optimization by using LLM

        Args:
            context (Dict[any, any]): dictionary, containing necessary information to form a prompt

        Returns
            str: optimized code 
        """
        prompt = generate_code_opt_with_one_hint(src_code=context["src_code"], other_codes=other_codes)
        # prompt = self.generate_prompt(context=context, task="CODE")
        #self.logger.info("Optimizer Prompt: \n%s", prompt)
        #self.logger.info("OPTIMIZER PROMPT END!!!")
        output, in_tokens, out_tokens = self.infer(prompt=prompt)
        optimized_code = clean_output(output)
        if optimized_code == "":
            print(f"No code block found for {self.model_name}")
        return optimized_code, in_tokens, out_tokens


    def generate_lesson(self, context: dict) -> str:
        """
        Generate Lesson by using LLM

        Args:
            context: dictionary, containing necessary information to form a prompt

        Returns
            str: a lesson generated by LLM
        """

        prompt = self.generate_prompt(context=context, task="LESSON")
        #self.logger.debug("Lesson Prompt: \n%s", prompt)
        #self.logger.debug("Finished Lesson Prompt Logging.\n")
        #self.logger.info("Lesson Prompt: \n%s", prompt)
        output, in_tokens, out_tokens = self.reason(prompt=prompt, temperature=self.reason_temperature, top_p=0.95, frequency_penalty=0.5)
        #optimized_code = clean_output(output)
        
        return output, in_tokens, out_tokens
    
    def identify_lesson(self, context: dict) -> str:
        """
        Identify Lesson by using LLM, check whether this lesson is correct or not
        context: dictionary, containing necessary information to form a prompt

        return: lesson
        """
        prompt = self.generate_prompt(context=context, task="IDENTIFY")
        #self.logger.debug("Identy Lesson Prompt: \n%s", prompt)
        # if self.model_name == "deepseek-ai/deepseek-coder-7b-instruct-v1.5":
        #     print("Identy Lesson Prompt: ")
        #     print(prompt)
        #self.logger.debug("Finished Identify Lesson Prompt Logging.\n")
        output, in_tokens, out_tokens = self.reason(prompt=prompt, temperature=self.reason_temperature, top_p=0.95, frequency_penalty=0.5)
        return output, in_tokens, out_tokens
    
    def modify_lesson(self, context: dict) -> str:
        """
        Identify Lesson by using LLM, check whether this lesson is correct or not
        context: dictionary, containing necessary information to form a prompt

        return: lesson
        """
        prompt = self.generate_prompt(context=context, task="MODIFY")
        #self.logger.debug("Modify Lesson Prompt: \n%s", prompt)
        # if self.model_name == "deepseek-ai/deepseek-coder-7b-instruct-v1.5":
        #     print("Modify Lesson Prompt: ")
        #     print(prompt)
        #print("Modify Lesson Prompt: ", prompt)
        #self.logger.debug("Finished Modify Prompt Logging.\n")
        output, in_tokens, out_tokens = self.reason(prompt=prompt, temperature=self.reason_temperature, top_p=0.95, frequency_penalty=0.5)
        # extract explanantion bounded in <explanation> and </explanation>
        # output = extract_first_explanation(output)
        return output, in_tokens, out_tokens
    
    def summarize_explanations(self, explanations: str) -> str:
        prompt = summary_thoughts(input=explanations)
        #self.logger.debug("Summarize Explanation Prompt :\n%s", prompt)
        output, in_tokens, out_tokens = self.reason(prompt=prompt, temperature=self.reason_temperature, top_p=0.95, frequency_penalty=0.5)
        
        return output, in_tokens, out_tokens
    
    def save_memory(self):
        save_name = f"{self.model_name}_memory.jsonl"
        save_name = save_name.split('/')[-1]
        # Write to JSON Lines file
        with jsonlines.open(save_name, mode="w") as writer:
            for round in self.memory:
                writer.write_all(round)